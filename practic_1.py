'''
–ó–∞–¥–∞—á–∞ 1. –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ñ–∞–ª—å—à–∏–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
–§–∞–ª—å—à–∏–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ ‚Äî —ç—Ç–æ –ª–æ–∂–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ–º–∞—è —á–µ—Ä–µ–∑
—Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏ –∏ –¥—Ä—É–≥–∏–µ —Å–µ—Ç–µ–≤—ã–µ –°–ú–ò –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö
–∏–ª–∏ –∏–¥–µ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ü–µ–ª–µ–π.

–¢–≤–æ—è  –∑–∞–¥–∞—á–∞ -  –∏—Å–ø–æ–ª—å–∑—É—è –±–∏–±–ª–∏–æ—Ç–µ–∫—É sklearn –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –º–æ–¥–µ–ª—å
–∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –º–æ–∂–µ—Ç —Å –≤—ã—Å–æ–∫–æ–π
—Ç–æ—á–Ω–æ—Å—Ç—å—é –±–æ–ª–µ–µ 90% –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –Ω–æ–≤–æ—Å—Ç—å —Ä–µ–∞–ª—å–Ω–æ–π (REALÔºâ
–∏–ª–∏ —Ñ–∞–ª—å—à–∏–≤–æ–πÔºàFAKE).

–¢—ã –¥–æ–ª–∂–µ–Ω —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –∏–∑—É—á–∏—Ç—å –∏ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –∫ –∑–∞–¥–∞—á–µ
TfidfVectorizer –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö
–¥–∞–Ω–Ω—ã—Ö –∏ PassiveAggressiveClassifier.

–ü–æ—Å—Ç—Ä–æ–π –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫ (confusion matrix).
–ü—Ä–µ–¥—Å—Ç–∞–≤—å, —á—Ç–æ –≤–∞—à –∑–∞–∫–∞–∑—á–∏–∫ –æ—á–µ–Ω—å –ª—é–±–∏—Ç –≥—Ä–∞—Ñ–∏–∫–∏ –∏ –¥–∏–∞–≥—Ä–∞–º–º—ã.
–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–π –¥–ª—è –Ω–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–∞–º, –≥–¥–µ —ç—Ç–æ –≤–æ–∑–º–æ–∂–Ω–æ.
'''

import re

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from pandas import pivot_table
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—Ç–∏–ª–µ–π –≥—Ä–∞—Ñ–∏–∫–æ–≤
plt.style.use('seaborn-v0_8')
sns.set_palette('husl')

def LoadAnalizeData(file_path):
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    try:
        df = pd.read_csv('E:/pyton/practic_1/data_fake_news.csv')
        print('–¥–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω')
    except Exception as e:
        print(f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}')

    print(f"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}")
    print(f"–ö–æ–ª–æ–Ω–∫–∏: {df.columns.tolist()}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω—É–ª–∏
    print(df.isnull().sum())

    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –≤ —Ç–µ–∫—Å—Ç–µ
    df = df.dropna(subset=['text'])
    print(f"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è: {df.shape}")

    return df

def preprocessorTextData(df):
    def preprocess_text(text):
        """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞"""
        if pd.isna(text):
            return ""
        text = str(text).lower()
        text = re.sub(r'[^a-zA-Z–∞-—è–ê-–Ø\s]', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    df['cleaned_text'] = df['text'].apply(preprocess_text)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
    print("–ü—Ä–∏–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏:")
    for i in range(2):
        print(f"–û—Ä–∏–≥–∏–Ω–∞–ª {i + 1}: {df['text'].iloc[i][:100]}")
        print(f"–û—á–∏—â–µ–Ω–Ω—ã–π {i + 1}: {df['cleaned_text'].iloc[i][:100]}")
        print()

    return df

def figure(df, plot_type, title, x_field=None, y_field=None, rotation=45, figsize=(10, 6)):
    plt.figure(figsize=figsize)

    if plot_type == 'bar':
        # –°—Ç–æ–ª–±—á–∞—Ç–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞
        if y_field is None:
            data = df[x_field].value_counts()
            x = data.index
            y = data.values
        else:
            data = df.groupby(x_field)[y_field].sum()
            x = data.index
            y = data.values
        bars = plt.bar(x, y)
        plt.title(title)
        plt.xlabel(x_field)
        plt.ylabel(y_field)
        plt.xticks(rotation=rotation)

        for bar in bars:
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width() / 2., height, f'{int(height)}', ha="center", va="bottom")

    elif plot_type == 'pie':
        # –ö—Ä—É–≥–æ–≤–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞
        if y_field is None:
            data = df[x_field].value_counts()
        else:
            data = df.groupby(x_field)[y_field].sum()

        plt.pie(data.values, labels=data.index, startangle=90)
        plt.title(title)
        plt.xlabel(x_field)
        plt.ylabel(y_field)

    elif plot_type == 'heatmap':
        # –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞
        if x_field and y_field:
            pivot_table = df.pivot_table(index=x_field, columns=y_field, aggfunc='size', fill_value=0)
            sns.heatmap(pivot_table, annot=True, fmt='d', cmap='YlOrRd')
            plt.title(title)
            plt.xlabel(y_field)
            plt.ylabel(x_field)
        else:
            # –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            numeric_df = df.select_dtypes(include=[np.number])
            if not numeric_df.empty:
                corr_matrix = numeric_df.corr()
                sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)
                plt.title(f'{title}\n(–ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏)')
            else:
                print("–ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏")
                return

    elif plot_type == 'confusion_matrix':
        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        if 'y_true' in df.columns and 'y_pred' in df.columns:
            cm = confusion_matrix(df['y_true'], df['y_pred'])
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
            plt.title(title)
            plt.xlabel(x_field)
            plt.ylabel(y_field)
        else:
            print("–î–ª—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ –Ω—É–∂–Ω—ã –∫–æ–ª–æ–Ω–∫–∏ 'y_true' –∏ 'y_pred'")
            return

    else:
        print(f'–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –≥—Ä–∞—Ñ–∏–∫–∞ {plot_type}')
        return

    plt.tight_layout()
    plt.show()

def main():
    # –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
    print(f'=' * 60)
    print('–ù–∞–π–¥–µ–º —Ñ–∞–ª—å—à–∏–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏')
    print(f'=' * 60)

    # –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = LoadAnalizeData('E:/pyton/PythonProject/data_fake_news.csv')
    if df is None:
        return
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    df = preprocessorTextData(df)

    # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
    label_counts = df['label'].value_counts()
    print(label_counts)

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    figure(df, 'bar', '–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∫–ª–∞—Å—Å–∞–º',
                x_field='label', rotation=0)

    figure(df, 'pie', '–ü—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤',
                x_field='label')

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â–∏–µ –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    X = df['cleaned_text']
    y = df['label']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape[0]} –∑–∞–ø–∏—Å–µ–π")
    print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape[0]} –∑–∞–ø–∏—Å–µ–π")

    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ TF-IDF
    tfidf_vectorizer = TfidfVectorizer(
        max_features=5000,
        stop_words='english',
        ngram_range=(1, 2),
        min_df=2,
        max_df=0.8
    )

    # –û–±—É—á–µ–Ω–∏–µ TF-IDF –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
    X_test_tfidf = tfidf_vectorizer.transform(X_test)

    print(f"üìê –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å TF-IDF –º–∞—Ç—Ä–∏—Ü—ã (train): {X_train_tfidf.shape}")
    print(f"üìê –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å TF-IDF –º–∞—Ç—Ä–∏—Ü—ã (test): {X_test_tfidf.shape}")


    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ PassiveAggressiveClassifier
    pac = PassiveAggressiveClassifier(
        max_iter=1000,
        random_state=42,
        C=0.5,
        early_stopping=True
    )

    pac.fit(X_train_tfidf, y_train)

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_pred = pac.predict(X_test_tfidf)

    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
    accuracy = accuracy_score(y_test, y_pred)
    print(f"–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ú–û–î–ï–õ–ò")
    print(f"–¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {accuracy:.4f} ({accuracy * 100:.2f}%)")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–µ–≤–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
    if accuracy >= 0.90:
        print("–¶–µ–ª–µ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å 90% –î–û–°–¢–ò–ì–ù–£–¢–ê!")
    else:
        print("–¶–µ–ª–µ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å 90% –ù–ï –î–û–°–¢–ò–ì–ù–£–¢–ê")

    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    cm = confusion_matrix(y_test, y_pred)
    print(f"–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫:")
    print(cm)

    # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—à—É —Ñ—É–Ω–∫—Ü–∏—é
    cm_df = pd.DataFrame({
        'y_true': y_test,
        'y_pred': y_pred
    })

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ —Å –ø–æ–º–æ—â—å—é –Ω–∞—à–µ–π —Ñ—É–Ω–∫—Ü–∏–∏
    figure(cm_df, 'confusion_matrix', '–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏')

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è: –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞
    print("–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
    feature_names = tfidf_vectorizer.get_feature_names_out()

    if hasattr(pac, 'coef_'):
        coefficients = pac.coef_[0]
        # –°–æ–∑–¥–∞–µ–º DataFrame —Å –≤–∞–∂–Ω–æ—Å—Ç—å—é —Å–ª–æ–≤
        feature_importance = pd.DataFrame({
            'word': feature_names,
            'importance': coefficients
        })

        # –¢–æ–ø-10 —Å–ª–æ–≤ –¥–ª—è FAKE –∏ REAL
        top_fake = feature_importance.nlargest(10, 'importance')
        top_real = feature_importance.nsmallest(10, 'importance')

        print("üìù –¢–æ–ø-10 —Å–ª–æ–≤ –¥–ª—è FAKE –Ω–æ–≤–æ—Å—Ç–µ–π:")
        print(top_fake[['word', 'importance']])

        print("\nüìù –¢–æ–ø-10 —Å–ª–æ–≤ –¥–ª—è REAL –Ω–æ–≤–æ—Å—Ç–µ–π:")
        print(top_real[['word', 'importance']])

        # –û—Ç—á–µ—Ç –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    print("\nüìã –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
    print(classification_report(y_test, y_pred, target_names=['FAKE', 'REAL']))

    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö
    print("\nüß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê –ü–†–ò–ú–ï–†–ê–•:")
    test_samples = [
        "Scientists discovered a new cancer treatment in clinical trials",
        "The government is hiding the truth about alien invasion",
        "Economic growth showed positive dynamics this quarter",
        "Secret method to lose 10 kg in 3 days without diet"
    ]

    for i, text in enumerate(test_samples, 1):
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        cleaned_text = re.sub(r'[^a-zA-Z–∞-—è–ê-–Ø\s]', '', text.lower())
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ TF-IDF –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        text_tfidf = tfidf_vectorizer.transform([cleaned_text])
        prediction = pac.predict(text_tfidf)[0]

        print(f"{i}. '{text[:50]}...' ‚Üí {prediction}")

    print("\nüéâ –ü–†–û–ì–†–ê–ú–ú–ê –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–ê!")
# –ó–∞–ø—É—Å–∫ –ø—Ä–æ–≥—Ä–∞–º–º—ã
if __name__ == "__main__":
    main()
